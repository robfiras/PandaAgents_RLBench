# --- DO NOT MODIFY THIS FILE -> MAKE A COPY OF IT TO MAKE YOUR OWN CONFIGURATION ---

# used by all agents
Agent:
  Type: OpenAIES # either DDPG, TD3 or OpenAIES (more coming soon)
  ArmActionMode: ABS_JOINT_VELOCITY # c.f., rlbench to see available arm action modes
  GripperActionMode: OPEN_AMOUNT  # c.f., rlbench to see available gripper action modes
  Observations:
    left_shoulder_camera: no
    right_shoulder_camera: no
    wrist_camera: no
    front_camera: yes
    joint_velocities: yes
    joint_positions: yes
    joint_forces: yes
    gripper_open: yes
    gripper_pose: yes
    gripper_joint_positions: yes
    gripper_touch_forces: yes
    record_gripper_closing: no
    task_low_dim_state: yes
  Setup:
    mode: online_training # either "online_training", "offline_training" or "validation"
    root_log_dir: null  # main logging directory, where models, tensorboard logs and buffers are saved
    run_id: null  # if not set, default run_id with time stamp is created
    load_model_run_id: null # run_id of the model in root_log_dir, whose parameters should be read
    use_tensorboard: no
    save_weights: no
    headless: no
    save_weights_interval: 10 # save every 10 episodes | For evolution strategies this is equal to "episodes_per_batch"
    seed: 94
    episodes: 40
    episode_length: 40
    logging_interval: 10
    scale_robot_observations: yes # if set robot observations are normalized
    save_camera_input: yes # if set all enabled cameras inputs are saved at root log dir
    domain_randomization_environment: no  # if no, domain_randomization_setup has no effect
    domain_randomization_setup:
      image_directory: null
      randomize_every: episode # either episode, variation or transition
    make_validation_during_training: no # if no validation_in_training_setup has no effect
    validation_in_training_setup:
      validation_interval: 1000 # interval of training episodes after which validation is done
      n_validation_episodes: 1000 # number of episodes for validation
    use_redundancy_resolution: no
    redundancy_resolution_setup:
      ref_position:
        - 0.0
        - -0.82
        - 0.0
        - -2.15
        - 0.0
        - 1.75
        - 0.75
      alpha: 1.0
  Task: PickAndLift # until now only ReachTarget or PickAndLift (more coming soon)

# config for all RL agents
RLAgent:
  Setup:
    n_workers: 4

# config for DDPG agents
DDPG:
  Setup:
    read_buffer_id: null # run_id of the model, whose buffer should be read
    write_buffer: no
    replay_buffer_mode: VANILLA # either VANILLA, PER_PYTHON or PER_CPP (more coming soon)
    use_target_copying: yes # use target copying instead of "soft" updates
    interval_copy_target: 300
    start_training: 500000  # steps before training starts
    buffer_size: 1000000
    use_ou_noise: no # use ou noise instead of Gaussian noise
    save_dones_in_buffer: no  # if false, task is continuous
    use_fixed_importance_sampling: no # if true, importance_sampling_weight is set, else calculated automatically
    importance_sampling_weight: 1.0

  Hyperparameters:
    gamma: 0.99
    tau: 0.001
    sigma: 0.4
    batch_size: 100
    training_interval: 1  # interval of training -> 1 = train in each step
    min_epsilon: 0.2
    max_epsilon: 0.9
    epsilon_decay_episodes: null # episodes in which epsilon decay from max to min. If not set, all training episodes
    lr_actor: 0.001
    lr_critic: 0.001
    layers_actor:
      - 400
      - 300
    layers_critic:
      - 400
      - 300
    # only for TD3:
    policy_stddev: 0.1 # regularization noise added to actions during training -> different from sigma!
    actor_noise_clipping: 0.5
    actor_update_frequency: 20

# config for all ES agents
ESAgent:
  Hyperparameters:
    n_workers: 2
    perturbations_per_batch: 1000
    episodes_per_perturbation: 1   # number of episode each perturbation is evaluated for
    weight_decay: 0.001
    return_mode: centered_ranks   # either plain_rewards, utility_function or centered_ranks
    lr: 0.01
    sigma: 0.05
    layers_network:
      - 400
      - 300

# some Panda specific parameters
Robot:
  keep_gripper_open: no # disables gripper action and keeps gripper open
  Dimensions:
    observations: 47
    actions: 8
  Limits:
    joint_vel:
      - 2.175
      - 2.175
      - 2.175
      - 2.175
      - 2.609
      - 2.609
      - 2.609
    lower_joint_pos:
      - -166.0
      - -101.0
      - -166.0
      - -176.0
      - -166.0
      - -1.0
      - -166.0
    range_joint_pos:
      - 332.0
      - 202.0
      - 332.0
      - 172.0
      - 332.0
      - 216.0
      - 332.0
    joint_force:
      - 87.0
      - 87.0
      - 87.0
      - 87.0
      - 12.0
      - 12.0
      - 12.0
    gripper_pose:
      - 3.0
      - 3.0
      - 3.0
      - 3.0
      - 1.0
      - 1.0
      - 1.0
    gripper_pos:
      - 0.08
      - 0.08
    gripper_force:
      - 20.0
      - 20.0
      - 20.0
      - 20.0
      - 20.0
      - 20.0
    gripper_open:
      - 1.0
    actions:
      - 1.0
      - 1.0
      - 1.0
      - 1.0
      - 1.0
      - 1.0
      - 1.0

# setup the camera (if selected above)
CameraConfig:
  left_shoulder_camera:
    rgb: yes
    depth: no
    mask: no
    mask_as_one_channel: yes
    image_size:
      - 128
      - 128
  right_shoulder_camera:
    rgb: yes
    depth: no
    mask: no
    mask_as_one_channel: yes
    image_size:
      - 128
      - 128
  wrist_camera:
    rgb: yes
    depth: no
    mask: no
    mask_as_one_channel: yes
    image_size:
      - 128
      - 128
  front_camera:
    rgb: yes
    depth: no
    mask: no
    mask_as_one_channel: yes
    image_size:
      - 128
      - 128









